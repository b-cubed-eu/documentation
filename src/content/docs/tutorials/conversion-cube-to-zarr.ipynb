{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "addf9593-6f98-4a65-9e84-2703bf64ae97",
   "metadata": {},
   "source": [
    "# Using B3 data cubes on AWS S3 storage\n",
    "This tutorial is going step by step over the creation of two additional data formats to store the B3 data cubes. We are focusing on GeoParquet and Zarr as potential candidates that contain the geometry within the files. 0\n",
    "\n",
    "In this example, we are using the GPKG files for Extended Quarter Degree Grid Cells that were created by GBIF: https://download.gbif.org/grids/EQDGC/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c388e-1f95-4d82-a1be-967c3542bb4d",
   "metadata": {},
   "source": [
    "## Initializing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8081ef-19de-458f-ae2a-625c14a9956b",
   "metadata": {},
   "source": [
    "### Loading the Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b11f48-1c66-4176-9fe4-0d83996ceb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "import zarr\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c78fa-0b58-4df7-a98d-416e31ad107c",
   "metadata": {},
   "source": [
    "### Setting environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535410f7-b002-40c9-9741-86819c458795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths to the stored files\n",
    "gbif_cube = \"/location/of/gbif/download.csv\"\n",
    "geometry_file = \"/location/of/grid.gpkg\"\n",
    "\n",
    "'''\n",
    "REMARK: in this example we use pre-generated GPKG files of the geospacial grids.\n",
    "However, it is possible to generate this file from any geospatial file format to GPKG using GDAL.\n",
    "'''\n",
    "\n",
    "# Evironment variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"your AWS access key ID\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"your AWS secret access key\"\n",
    "\n",
    "# S3 region\n",
    "s3_region = \"region\" # e.g. eu-north-1\n",
    "\n",
    "# Location to which the GeoParquet file needs to be stored\n",
    "geoparquet_path = \"/path/to/cube.parquet\"\n",
    "\n",
    "# Link to S3 Bucket to store the Zarr file\n",
    "s3_path = \"s3://your/S3/bucket/cubeName.zarr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275939ca-2e32-470d-b0c9-2ed45d4223ff",
   "metadata": {},
   "source": [
    "## Loading the data in GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a17f60-215e-4a07-b639-adcdc9a87bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "data = pd.read_csv(gbif_cube, sep='\\t')\n",
    "\n",
    "# Load GRID (Geopackage)\n",
    "qdgc_ref = gpd.read_file(geometry_file, engine='pyogrio', use_arrow=False)\n",
    "\n",
    "# Ensure CRS is set (modify CRS if another datum is used!)\n",
    "if qdgc_ref.crs is None:\n",
    "    qdgc_ref.set_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "# Merge Data, in this step you need to check the columns on which to perform the matching\n",
    "test_merge = pd.merge(data, qdgc_ref, left_on='eqdgccellcode', right_on='cellCode')\n",
    "gdf = gpd.GeoDataFrame(test_merge, geometry='geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f15fb1-35ad-41f5-b570-0ab4bd4ef7ea",
   "metadata": {},
   "source": [
    "## Exporting the data to GeoParquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e94891-b98c-4621-b37c-51ebe2f17b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_parquet(geoparquet_path, geometry_encoding='WKB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6797c-b63c-4fe1-829b-6df1a8aeb750",
   "metadata": {},
   "source": [
    "## Exporting the data to Zarr in an AWS S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0346b75-1c42-42a6-9a69-6937b927e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dask DataFrame\n",
    "ddf = dd.from_pandas(gdf, npartitions=max(1, gdf[\"specieskey\"].nunique() // 1000))  # Dynamic partitioning\n",
    "columns_to_compute = [\"yearmonth\", \"eqdgccellcode\", \"familykey\", \"family\", \"specieskey\",\n",
    "                      \"species\", \"occurrences\", \"distinctobservers\",\n",
    "                      \"familycount\", \"geometry\"]\n",
    "\n",
    "pdf = ddf[columns_to_compute].compute()\n",
    "\n",
    "# Ensure geometry is still a GeoSeries before conversion\n",
    "if not isinstance(pdf[\"geometry\"], gpd.GeoSeries):\n",
    "    pdf[\"geometry\"] = gpd.GeoSeries(pdf[\"geometry\"], crs=\"EPSG:4326\")\n",
    "\n",
    "# Convert geometry column to WKT (text format for serialization)\n",
    "pdf[\"geometry\"] = pdf[\"geometry\"].apply(lambda geom: geom.wkt if geom and geom is not None else \"\")\n",
    "\n",
    "# Ensure all other columns have appropriate types\n",
    "for col in pdf.columns:\n",
    "    if pdf[col].dtype.name == \"string[pyarrow]\":  \n",
    "        pdf[col] = pdf[col].astype(str).fillna(\"\")  # Convert to string and replace NaN\n",
    "    elif pdf[col].dtype.kind in ['i', 'f']:  \n",
    "        pdf[col] = pdf[col].fillna(0)  # Replace NaN with 0 for numbers\n",
    "    elif pdf[col].dtype == \"object\":  \n",
    "        pdf[col] = pdf[col].astype(str).fillna(\"\")  # Ensure object columns are converted to string\n",
    "\n",
    "# Convert to Xarray\n",
    "ds = xr.Dataset.from_dataframe(pdf)\n",
    "ds = ds.chunk({\"index\": 10000})  # Optimize chunking for large datasets\n",
    "\n",
    "# S3 Config\n",
    "s3_kwargs = {\n",
    "    \"key\": os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    \"secret\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "}\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'region_name': s3_region})  \n",
    "\n",
    "# Ensure no existing file conflict\n",
    "if fs.exists(s3_path):\n",
    "    fs.rm(s3_path, recursive=True)\n",
    "\n",
    "# Save to Zarr (Local Write First, Then Move to S3)\n",
    "try:\n",
    "    ds.to_zarr(\"local_temp.zarr\", mode=\"w\")\n",
    "    fs.put(\"local_temp.zarr\", s3_path, recursive=True, batch_size=50)\n",
    "    print(\"Zarr store written to S3 successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to Zarr: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
